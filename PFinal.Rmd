---
title: "Práctica Final"
author: 
  - "© Gerardo Tirado García"
  - "© Irene Trigueros Lorca"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    number_sections: no
    toc_float:
      collapsed: yes
      smooth_scroll: yes
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document: default
---
<style>
.math{
  font-size: 8.25pt;options(encoding = 'UTF-8')
}
</style>  

<div style="text-align: justify">
---
En esta práctica vamos a realizar un análisis exploratorio (tanto univariante como multivariante) de una de las bases de datos propuestas. 
En nuestro caso, elegiremos la base de datos 3: En el conjunto constituido por 34 estados del mundo, se han observado 11 variables cuyos resultados se recogen en el archivo DB_3.sav. 
Estas variables ya se encuentran estandarizadas, pues están tomadas con unidades de medida muy diferentes. 
Estas variables son:

* Ztlibrop: Número de libros publicados
* Ztejerci: Cociente entre el número de individuos en ejército de tierra y población total del estado.
* Ztpobact: Cociente entre población activa y total.
* Ztenergi: Tasa de consumo energético.
* Zpservi: Población del sector servicios. 
* Zpagricu: Población del sector agrícola.
* Ztmedico: Tasa de médicos por habitante. 
* Zespvida: Esperanza de vida. 
* Ztminfan: Tasa de mortalidad infantil.
* Zpobdens: Densidad de población.

# **Instalación y carga de librerías**

```{r echo=TRUE, include=TRUE, warning=FALSE}
# Instalamos las librerías a utilizar si no están ya instaladas
#install.packages("psych")
#install.packages("polycor")
#install.packages("ggcorrplot")
#install.packages("corrr")
#install.packages("moments") #skewness
#install.packages("dplyr") #Test de Saphiro-Wilk
#install.packages("factoextra")
#Cargamos las librerías que vamos a utilizar
library(psych)
library(polycor)
library(ggcorrplot)
library(corrplot)
library(corrr)
library(moments)
library(foreign)
library(reshape2)
library(knitr)
library(dplyr)
library(ggplot2)
library("factoextra")
library(stats)
```

Cargamos los datos de la base de datos mencionados anteriormente:
```{r echo=TRUE, include=TRUE, warning=FALSE}
datos_original<-read.spss("DB_3.sav", to.data.frame = TRUE)
```

# **Análisis exploratorio univariante**

## *Recodificaciones o agrupaciones de datos*

En primer lugar, eliminamos la primera columna, que representa a los países, ya 
que no nos sirven:

```{r echo=TRUE, include=TRUE, warning=FALSE}
datos<-datos_original[,-1]
```

## *Valores perdidos*

Definimos la función que nos permite identificar el porcentaje de valores perdidos
de cada variable:

```{r echo=TRUE, include=TRUE, warning=FALSE}
per_not_available<-function(data, na.rm=F){
  colMeans(is.na(data))*100
}
per_not_available(datos)
```

Podemos observar que ninguna variable tiene valores perdidos, excepto `ZTLIBROP`,
con un 2.941176% de valores perdidos (lo que se corresponde en un solo valor 
perdido).

Como en este paso solo tenemos que tratar con las variables que tengan más de 
un 5% de valores perdidos, y no tenemos ningún caso en el que esto pase, 
no hay más que hacer. 
Sin embargo, la mayor parte del análisis que tenemos que hacer requiere de que 
no haya valores perdidos (por ejemplo, para la correlación si hay valores 
perdidos, las filas y las columnas de las variables con  valores perdidos se 
llenarán de `NA`); por lo tanto, trataremos con estos valores ahora sustituyéndolos 
por la media de la variable:

```{r echo=TRUE, include=TRUE, warning=FALSE}
not_available<-function(data,na.rm=F){
  data[is.na(data)]<-mean(data,na.rm=T)
  data
}
datos$ZTLIBROP<-not_available(datos$ZTLIBROP)
```

## *Análisis descriptivo numérico clásico*

Podemos observar los siguientes valores característicos:

* Mínimo
* Primer cuartil
* Mediana
* Media
* Tercer cuartil
* Máximo

de cada variable utilizando la función `summary`:

```{r echo=TRUE, include=TRUE, warning=FALSE}
summary(datos)
```

Podemos comprobar la simetría de cada variable utilizando la función `skewness`:

```{r echo=TRUE, include=TRUE, warning=FALSE}
skewness(datos) #https://www.programmingr.com/statistics/skewness/ 
```

Sabemos que si la media se encuentra a la derecha de la mediana, obtendremos un 
skewness positivo; en caso contrario, obtendremos un skewness negativo.

Podemos comprobar gráficamente la simetría de las variables utilizando: 

```{r echo=TRUE, include=TRUE, warning=FALSE}
hist(x = datos$ZTLIBROP)
hist(x = datos$ZTEJERCI)
hist(x = datos$ZTPOBACT)
hist(x = datos$ZTENERGI)
hist(x = datos$ZPSERVI)
hist(x = datos$ZPAGRICU)
hist(x = datos$ZTMEDICO)
hist(x = datos$ZESPVIDA)
hist(x = datos$ZTMINFAN)
hist(x = datos$ZPOBDENS)
hist(x = datos$ZPOBURB)
```

La curtosis mide cómo de achatada o apuntada es la curva y cómo se agrupan 
valores en torno a la media (la curtosis de una distribución normal es 3):

* La curtosis de una distribución normal es 3.
* Si una distribución dada tiene una curtosis menor que 3, se dice que es *playkurtic*, lo que significa que tiende a producir menos valores atípicos y menos extremos que la distribución normal.
* Si una distrubicón dada tiene una curtosis mayor que 3, se dice que es *leptocúrtica*, lo que significa que tiende a producir más valores atípicos que la distribución normal.

Se puede calcular utilizando la función `kurtosis`:

```{r echo=TRUE, include=TRUE, warning=FALSE}
kurtosis(datos) #https://statologos.com/asimetria-curtosis-en-r/
```

## *Valores extremos (outliers)*

El objetivo es el de localizar outliers que puedan dar lugar a resultados
erróneos ya que el ACP es muy sensible a valores extremos. Un diagrama de 
cajas puede dar esta primera información:

```{r echo=TRUE, include=TRUE, warning=FALSE}
boxplot(datos,main="Análisis de outliers",
        xlab="Variables sociodemográficas",
        ylab="z-values",
        col=c(1:11))
```

Al obtener el gráfico con estos datos, se observa que tanto `ZOPBENDS`, 
`ZJEJERCI` y `ZTENERGI` presentan outliers.

Los outliers deben ser tratados de forma independiente por el investigador, de 
modo que para el APC es necesario eliminarlos. 
La función outlier definida como sigue elimina los outliers sustituyéndolos por....

```{r echo=TRUE, include=TRUE, warning=FALSE}
outlier<-function(data,na.rm=T){
  H<-1.5*IQR(data)
  data[data<quantile(data,0.25,na.rm = T)-H]<-NA
  data[data>quantile(data,0.75, na.rm = T)+H]<-NA
  data[is.na(data)]<-mean(data, na.rm = T)
  H<-1.5*IQR(data)
  if (TRUE %in% (data<quantile(data,0.25,na.rm = T)-H) | TRUE %in% (data>quantile(data,0.75,na.rm = T)+H))
    outlier(data)
  else
    return(data)
}
```

Entonces aplicamos esta función a las variables donde hemos encontrado outliers:

```{r echo=TRUE, include=TRUE, warning=FALSE}
datos$ZPOBDENS <- outlier(datos$ZPOBDENS)
datos$ZTENERGI <- outlier(datos$ZTENERGI)
datos$ZTEJERCI <- outlier(datos$ZTEJERCI)
```

Y volvemos a mostrar el diagrama de cajas con estos nuevos cambios: 

```{r echo=TRUE, include=TRUE, warning=FALSE}
boxplot(datos,main="Análisis de outliers",
        xlab="Variables sociodemográficas",
        ylab="z-values",
        col=c(1:11))
```

Al obtener el gráfico con estos datos, se observa que ya ninguna variable 
presenta outliers.

## *Supuesto de normalidad*

### Normalidad con gráficas qqplot

```{r echo=TRUE, include=TRUE, warning=FALSE}
qqnorm(datos$ZTLIBROP, main="ZTLIBROP", pch=1, frame = FALSE)
qqline(datos$ZTLIBROP, col = 'blue', lwd=2)
qqnorm(datos$ZTEJERCI, main="ZTEJERCI", pch=1, frame = FALSE)
qqline(datos$ZTEJERCI, col = 'blue', lwd=2)
qqnorm(datos$ZTPOBACT, main="ZTPOBACT", pch=1, frame = FALSE)
qqline(datos$ZTPOBACT, col = 'blue', lwd=2)
qqnorm(datos$ZTENERGI, main="ZTENERGI", pch=1, frame = FALSE)
qqline(datos$ZTENERGI, col = 'blue', lwd=2)
qqnorm(datos$ZPSERVI, main="ZPSERVI", pch=1, frame = FALSE)
qqline(datos$ZPSERVI, col = 'blue', lwd=2)
qqnorm(datos$ZPAGRICU, main="ZPAGRICU", pch=1, frame = FALSE)
qqline(datos$ZPAGRICU, col = 'blue', lwd=2)
qqnorm(datos$ZTMEDICO, main="ZTMEDICO", pch=1, frame = FALSE)
qqline(datos$ZTMEDICO, col = 'blue', lwd=2)
qqnorm(datos$ZESPVIDA, main="ZESPVIDA", pch=1, frame = FALSE)
qqline(datos$ZESPVIDA, col = 'blue', lwd=2)
qqnorm(datos$ZTMINFAN, main="ZTMINFAN", pch=1, frame = FALSE)
qqline(datos$ZTMINFAN, col = 'blue', lwd=2)
qqnorm(datos$ZPOBDENS, main="ZPOBDENS", pch=1, frame = FALSE)
qqline(datos$ZPOBDENS, col = 'blue', lwd=2)
qqnorm(datos$ZPOBURB, main="ZPOBURB", pch=1, frame = FALSE)
qqline(datos$ZPOBURB, col = 'blue', lwd=2)
```

### Test de Saphiro-Wilk

```{r echo=TRUE, include=TRUE, warning=FALSE}
datos_tidy <- melt(datos, value.name = "valor")
aggregate(valor ~ variable, data = datos_tidy,
          FUN = function(x){shapiro.test(x)$p.value})
#x>0.05 -> ZPOBURB, ZPSERVI, ZTEJERCI, ZTPOBACT 
qqnorm(datos$ZPOBURB, pch=1, frame = FALSE)
qqline(datos$ZPOBURB, col = 'blue', lwd=2)
```

## *Otras cuestiones de interés*

# **Análisis exploratorio multivariante**

## *Correlación*

```{r echo=TRUE, include=TRUE, warning=FALSE}
cor(datos)
corrplot(cor(datos), order = "hclust", tl.col='black', tl.cex=1)
```

La prueba de esfericidad de Bartlett contrasta la hipótesis nula de que la matriz 
de correlaciones es una matriz identidad, en cuyo caso no existirían correlaciones 
significativas entre las variables y el modelo factorial no sería pertinente. 

```{r echo=TRUE, include=TRUE, warning=FALSE}
cortest.bartlett(datos) #https://rpubs.com/PacoParra/293407
```

## *Valores perdidos*

Ya han sido tratados en el apartado del análisis univariante.

## *Posibilidad de reducción de la dimensión*

### *Mediante variables observables*

Es conveniente elegir el número óptimo de componentes principales. 
El siguiente código fuente realiza el ACP, obteniendo los vectores propios que generan cada componente, así como sus valores propios que corresponden a la varianza de cada una. 

La función "prcomp" del paquete base de R realiza este análisis. 
Pasamos los parámetros "scale" y "center" a `TRUE` para considerar los datos originales normalizados:

```{r echo=TRUE, include=TRUE, warning=FALSE}
# Realización del ACP
PCA<-prcomp(datos, scale=T, center = T)
```

El campo "rotation" del objeto "PCA" es una matriz cuyas columnas son los coeficientes de las componentes principales, es decir, el peso de cada variable en la correspondiente componente principal.

```{r echo=TRUE, include=TRUE, warning=FALSE}
PCA$rotation
```

En el campo "sdev" del objeto "PCA" y con la función summary aplicada al objeto, obtenemos información relevante: desviaciones típicas de cada componente principal, proporción de varianza explicada y acumulada:

```{r echo=TRUE, include=TRUE, warning=FALSE}
PCA$sdev
summary(PCA)
```

Los siguientes gráficos ilustran el comportamiento de la varianza explicada por cada componente principal, así como el comportamiento de la varianza explicada acumulada:

```{r echo=TRUE, include=TRUE, warning=FALSE}
# Proporción de varianza explicada
varianza_explicada <- PCA$sdev^2 / sum(PCA$sdev^2)
ggplot(data = data.frame(varianza_explicada, pc = 1:11),
       aes(x = pc, y = varianza_explicada, fill=varianza_explicada )) +
  geom_col(width = 0.3) +
  scale_y_continuous(limits = c(0,0.6)) + theme_bw() +
  labs(x = "Componente principal", y= " Proporción de varianza explicada")

# Proporción de varianza explicada acumulada
varianza_acum<-cumsum(varianza_explicada)
ggplot( data = data.frame(varianza_acum, pc = 1:11),
        aes(x = pc, y = varianza_acum ,fill=varianza_acum )) +
  geom_col(width = 0.5) +
  scale_y_continuous(limits = c(0,1)) +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Proporción varianza explicada acumulada")
```

#### Selección del número de componentes principales óptimo

Una vez hecho esto, existen diferentes métodos para elegir el número de componentes principales con las que trabajar: 

* Método del codo (Cuadras, 2007)
* A criterio del investigador que elige un porcentaje mínimo de varianza explicada por las componentes principales (no es fiable porque puede dar más de las necesarias).
* Regla de Abdi et al: Se promedia las varianzas explicadas por las componentes principales y se seleccionan aquellas cuya proporción de varianza explicada supera la media.

En este caso, utilizaremos tanto la Regla de Adbi et al como el criterio del investigador, eligiendo 4 componentes principales. 
Ya que las varianzas explicadas por solo 3 componentes supera le media, pero con la cuarta no se queda muy por debajo de la media y conseguimos alcanzar el 85% de la varianza explicada acumulada:

```{r echo=TRUE, include=TRUE, warning=FALSE}
PCA$sdev^2
mean(PCA$sdev^2)
```

Cada componente principal se obtiene de forma sencilla como combinación lineal de todas las variables con los coeficientes que indican las columnas de la matriz de rotación.

#### Representación gráfica de las componentes principales

Podemos mostrar las comparativas entre las distintas componentes principales analizando qué variables tienen más peso para la definición de cada componente principal utilizando la función `fviz_pca_var`:

```{r echo=TRUE, include=TRUE, warning=FALSE}
fviz_pca_var(PCA,
             repel=TRUE,col.var="cos2",
             legend.title="Distancia")+theme_bw()
fviz_pca_var(PCA,axes=c(1,3),
             repel=TRUE,col.var="cos2",
             legend.title="Distancia")+theme_bw()
fviz_pca_var(PCA,axes=c(2,3),
             repel=TRUE,col.var="cos2",
             legend.title="Distancia")+theme_bw()
fviz_pca_var(PCA,axes=c(1,4),
             repel=TRUE,col.var="cos2",
             legend.title="Distancia")+theme_bw()
fviz_pca_var(PCA,axes=c(2,4),
             repel=TRUE,col.var="cos2",
             legend.title="Distancia")+theme_bw()
fviz_pca_var(PCA,axes=c(3,4),
             repel=TRUE,col.var="cos2",
             legend.title="Distancia")+theme_bw()
```

Es posible también representar las observaciones de los objetivos junto con las componentes principales mediante la orden `contrib` de la función `fviz_pca_ind`, así como identificar con colores aquellas observaciones que mayor varianza explican de las componentes principales.

```{r echo=TRUE, include=TRUE, warning=FALSE}
fviz_pca_ind(PCA,col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var")+theme_bw()
fviz_pca_ind(PCA,axes=c(1,3),col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var")+theme_bw()
fviz_pca_ind(PCA,axes=c(2,3),col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var")+theme_bw()
fviz_pca_ind(PCA,axes=c(1,4),col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var")+theme_bw()
fviz_pca_ind(PCA,axes=c(2,4),col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var")+theme_bw()
fviz_pca_ind(PCA,axes=c(3,4),col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var")+theme_bw()
```

También mostraremos la representación conjunta de  variables y observaciones que relaciona visualmente las posibles relaciones entre las observaciones, las contribuciones de los individuos a las varianzas de las componentes y el peso de las variables en cada componente principal.

```{r echo=TRUE, include=TRUE, warning=FALSE}
fviz_pca(PCA,
         alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE,
         legend.title="Distancia")+theme_bw()
fviz_pca(PCA,axes=c(1,3),
         alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE,
         legend.title="Distancia")+theme_bw()
fviz_pca(PCA,axes=c(2,3),
         alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE,
         legend.title="Distancia")+theme_bw()
fviz_pca(PCA,axes=c(1,4),
         alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE,
         legend.title="Distancia")+theme_bw()
fviz_pca(PCA,axes=c(2,4),
         alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE,
         legend.title="Distancia")+theme_bw()
fviz_pca(PCA,axes=c(3,4),
         alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE,
         legend.title="Distancia")+theme_bw()
```

Por último, ya que el objeto de este estudio era reducir la dimensión de las variables utilizadas, es posible obtener las coordenadas de los datos originales tipificados en el nuevo sistema de referencia. 
De hecho, lo tenemos almacenados desde que utilizamos la función `prcomp` para crear la variable PCA:

```{r echo=TRUE, include=TRUE, warning=FALSE}
head(PCA$x,n=4)
```

### *Mediante variables latentes*
  
```{r echo=TRUE, include=TRUE, warning=FALSE}
poly_cor<-hetcor(datos)$correlations
```

Hay que escoger un método para extraer los factores, ACP, verosimilitud, etc. 
La función `fa()` implementa hasta 6 métodos distintos. 
Vamos a comprarar las salidas con el método del factor principal y con el de máxima verosimilitud.

```{r echo=TRUE, include=TRUE, warning=FALSE}
### prueba de dos modelos con tres factores
modelo1<-fa(poly_cor,
            nfactors = 4,
            rotate = "none",
            fm="mle") # modelo m?xima verosimilitud
modelo2<-fa(poly_cor,
            nfactors = 4,
            rotate = "none",
            fm="minres") # modelo m?nimo residuo
```

Comparamos las comunalidades:

```{r echo=TRUE, include=TRUE, warning=FALSE}
sort(modelo1$communality,decreasing = T)->c1
sort(modelo2$communality,decreasing = T)->c2
head(cbind(c1,c2))
```

Comparamos las unicidades, es decir, la proporción de varianza que no ha sido explicada por el factor (1-comunalidad)

```{r echo=TRUE, include=TRUE, warning=FALSE}
sort(modelo1$uniquenesses,decreasing = T)->u1
sort(modelo2$uniquenesses,decreasing = T)->u2
head(cbind(u1,u2))
```

Determinemos ahora el número óptimo de factores. 
Hay diferentes criterios, entre los que destacan el Scree plot y el análisis paralelo:

```{r echo=TRUE, include=TRUE, warning=FALSE}
scree(poly_cor)
fa.parallel(poly_cor,n.obs=200,fa="fa",fm="minres")
```

Estimamos el modelo factorial con 4 factores implementando una rotación tipo varimax para buscar una interpretación más simple:

```{r echo=TRUE, include=TRUE, warning=FALSE}
modelo_varimax<-fa(poly_cor,nfactors = 2,rotate = "varimax",
                   fa="mle")
```

Mostramos la mariz de pesos factorial rotada:

```{r echo=TRUE, include=TRUE, warning=FALSE}
print(modelo_varimax$loadings,cut=0) 
```

Visualmente podríamos hacer el esfuerzo de  ver con qué variables correlacionan cada uno de los factores, pero es muy tedioso; de modo que utilizamos la siguiente representación:

```{r echo=TRUE, include=TRUE, warning=FALSE}
fa.diagram(modelo_varimax)
```

En este diagrama, entre otras cosas, se ve que el primer factor está asociado con los items `ZPAGRICU`, `ZPOBURB`, `ZPSERVI`, `ZESPVIDA`, `ZTMINFAN`, `ZTLIBROP`, `ZTMEDICO`, `ZTENERGI`; mientras que el segundo factor está asociado solo al item `ZTPOPBACT`.

Otra forma de hacerlo es con un test de hipótesis al final que contrasta si el número de factores es suficiente:

```{r echo=TRUE, include=TRUE, warning=FALSE}
factanal(datos,factors=4, rotation="none")
```

## *Análisis de normalidad*
                                     
```{r echo=TRUE, include=TRUE, warning=FALSE}                                        
# El paquete solo se instala una vez para evitar errores de ejecución puesto que está en nuestro repositorio local.
install.packages("MVN")
library(MVN)
outliers <- mvn(data = datos, mvnTest = "hz", multivariateOutlierMethod = "quan")

royston_test <- mvn(data = datos[,-1], mvnTest = "royston", multivariatePlot = "qq")
royston_test$multivariateNormality

hz_test <- mvn(data = datos, mvnTest = "hz")
hz_test$multivariateNormality
  
 ```
                                     

## *Construcción de un clasificador*

## *Validación de los clasificadores*
