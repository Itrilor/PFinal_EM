---
title: "Práctica Final"
author: 
  - "© Gerardo Tirado García"
  - "© Irene Trigueros Lorca"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    number_sections: no
    toc_float:
      collapsed: yes
      smooth_scroll: yes
  word_document:
    toc: yes
    toc_depth: '3'
  pdf_document: default
---
<style>
.math{
  font-size: 8.25pt;options(encoding = 'UTF-8')
}
</style>  

<div style="text-align: justify">
---
En esta práctica vamos a realizar un análisis exploratorio (tanto univariante como multivariante) de una de las bases de datos propuestas. 
En nuestro caso, elegiremos la base de datos 3: En el conjunto constituido por 34 estados del mundo, se han observado 11 variables cuyos resultados se recogen en el archivo DB_3.sav. 
Estas variables ya se encuentran estandarizadas, pues están tomadas con unidades de medida muy diferentes. 
Estas variables son:

* Ztlibrop: Número de libros publicados
* Ztejerci: Cociente entre el número de individuos en ejército de tierra y población total del estado.
* Ztpobact: Cociente entre población activa y total.
* Ztenergi: Tasa de consumo energético.
* Zpservi: Población del sector servicios. 
* Zpagricu: Población del sector agrícola.
* Ztmedico: Tasa de médicos por habitante. 
* Zespvida: Esperanza de vida. 
* Ztminfan: Tasa de mortalidad infantil.
* Zpobdens: Densidad de población.

# **Instalación y carga de librerías**

```{r echo=TRUE, include=TRUE, warning=FALSE}
# Instalamos las librerías a utilizar si no están ya instaladas
#install.packages("psych")
#install.packages("polycor")
#install.packages("ggcorrplot")
#install.packages("corrr")
#install.packages("moments") #skewness
#install.packages("dplyr") #Test de Saphiro-Wilk
#Cargamos las librerías que vamos a utilizar
library(psych)
library(polycor)
library(ggcorrplot)
library(corrplot)
library(corrr)
library(moments)
library(foreign)
library(reshape2)
library(knitr)
library(dplyr)
```

Cargamos los datos de la base de datos mencionados anteriormente:
```{r echo=TRUE, include=TRUE, warning=FALSE}
datos_original<-read.spss("DB_3.sav", to.data.frame = TRUE)
```

# **Análisis exploratorio univariante**

## *Recodificaciones o agrupaciones de datos*

En primer lugar, eliminamos la primera columna, que representa a los países, ya 
que no nos sirven:

```{r echo=TRUE, include=TRUE, warning=FALSE}
datos<-datos_original[,-1]
```

## *Valores perdidos*

Definimos la función que nos permite identificar el porcentaje de valores perdidos
de cada variable:

```{r echo=TRUE, include=TRUE, warning=FALSE}
per_not_available<-function(data, na.rm=F){
  colMeans(is.na(data))*100
}
per_not_available(datos)
```

Podemos observar que ninguna variable tiene valores perdidos, excepto `ZTLIBROP`,
con un 2.941176% de valores perdidos (lo que se corresponde en un solo valor 
perdido).

Como en este paso solo tenemos que tratar con las variables que tengan más de 
un 5% de valores perdidos, y no tenemos ningún caso en el que esto pase, 
no hay más que hacer. 
Sin embargo, la mayor parte del análisis que tenemos que hacer requiere de que 
no haya valores perdidos (por ejemplo, para la correlación si hay valores 
perdidos, las filas y las columnas de las variables con  valores perdidos se 
llenarán de `NA`); por lo tanto, trataremos con estos valores ahora sustituyéndolos 
por la media de la variable:

```{r echo=TRUE, include=TRUE, warning=FALSE}
not_available<-function(data,na.rm=F){
  data[is.na(data)]<-mean(data,na.rm=T)
  data
}
datos$ZTLIBROP<-not_available(datos$ZTLIBROP)
```

## *Análisis descriptivo numérico clásico*

Podemos observar los siguientes valores característicos:

* Mínimo
* Primer cuartil
* Mediana
* Media
* Tercer cuartil
* Máximo

de cada variable utilizando la función `summary`:

```{r echo=TRUE, include=TRUE, warning=FALSE}
summary(datos)
```

Podemos comprobar la simetría de cada variable utilizando la función `skewness`:

```{r echo=TRUE, include=TRUE, warning=FALSE}
skewness(datos) #https://www.programmingr.com/statistics/skewness/ 
```

Sabemos que si la media se encuentra a la derecha de la mediana, obtendremos un 
skewness positivo; en caso contrario, obtendremos un skewness negativo.

Podemos comprobar gráficamente la simetría de las variables utilizando: 

```{r echo=TRUE, include=TRUE, warning=FALSE}
hist(x = datos$ZTLIBROP)
hist(x = datos$ZTEJERCI)
hist(x = datos$ZTPOBACT)
hist(x = datos$ZTENERGI)
hist(x = datos$ZPSERVI)
hist(x = datos$ZPAGRICU)
hist(x = datos$ZTMEDICO)
hist(x = datos$ZESPVIDA)
hist(x = datos$ZTMINFAN)
hist(x = datos$ZPOBDENS)
hist(x = datos$ZPOBURB)
```

La curtosis mide cómo de achatada o apuntada es la curva y cómo se agrupan 
valores en torno a la media (la curtosis de una distribución normal es 3):

* La curtosis de una distribución normal es 3.
* Si una distribución dada tiene una curtosis menor que 3, se dice que es *playkurtic*, lo que significa que tiende a producir menos valores atípicos y menos extremos que la distribución normal.
* Si una distrubicón dada tiene una curtosis mayor que 3, se dice que es *leptocúrtica*, lo que significa que tiende a producir más valores atípicos que la distribución normal.

Se puede calcular utilizando la función `kurtosis`:

```{r echo=TRUE, include=TRUE, warning=FALSE}
kurtosis(datos) #https://statologos.com/asimetria-curtosis-en-r/
```

## *Valores extremos (outliers)*

El objetivo es el de localizar outliers que puedan dar lugar a resultados
erróneos ya que el ACP es muy sensible a valores extremos. Un diagrama de 
cajas puede dar esta primera información:

```{r echo=TRUE, include=TRUE, warning=FALSE}
boxplot(datos,main="Análisis de outliers",
        xlab="Variables sociodemográficas",
        ylab="z-values",
        col=c(1:11))
```

Al obtener el gráfico con estos datos, se observa que tanto `ZOPBENDS`, 
`ZJEJERCI` y `ZTENERGI` presentan outliers.

Los outliers deben ser tratados de forma independiente por el investigador, de 
modo que para el APC es necesario eliminarlos. 
La función outlier definida como sigue elimina los outliers sustituyéndolos por....

```{r echo=TRUE, include=TRUE, warning=FALSE}
outlier<-function(data,na.rm=T){
  H<-1.5*IQR(data)
  data[data<quantile(data,0.25,na.rm = T)-H]<-NA
  data[data>quantile(data,0.75, na.rm = T)+H]<-NA
  data[is.na(data)]<-mean(data, na.rm = T)
  H<-1.5*IQR(data)
  if (TRUE %in% (data<quantile(data,0.25,na.rm = T)-H) | TRUE %in% (data>quantile(data,0.75,na.rm = T)+H))
    outlier(data)
  else
    return(data)
}
```

Entonces aplicamos esta función a las variables donde hemos encontrado outliers:

```{r echo=TRUE, include=TRUE, warning=FALSE}
datos$ZPOBDENS <- outlier(datos$ZPOBDENS)
datos$ZTENERGI <- outlier(datos$ZTENERGI)
datos$ZTEJERCI <- outlier(datos$ZTEJERCI)
```

Y volvemos a mostrar el diagrama de cajas con estos nuevos cambios: 

```{r echo=TRUE, include=TRUE, warning=FALSE}
boxplot(datos,main="Análisis de outliers",
        xlab="Variables sociodemográficas",
        ylab="z-values",
        col=c(1:11))
```

Al obtener el gráfico con estos datos, se observa que ya ninguna variable 
presenta outliers.

## *Supuesto de normalidad*

### Normalidad con gráficas qqplot

```{r echo=TRUE, include=TRUE, warning=FALSE}
qqnorm(datos$ZTLIBROP, main="ZTLIBROP", pch=1, frame = FALSE)
qqline(datos$ZTLIBROP, col = 'blue', lwd=2)
qqnorm(datos$ZTEJERCI, main="ZTEJERCI", pch=1, frame = FALSE)
qqline(datos$ZTEJERCI, col = 'blue', lwd=2)
qqnorm(datos$ZTPOBACT, main="ZTPOBACT", pch=1, frame = FALSE)
qqline(datos$ZTPOBACT, col = 'blue', lwd=2)
qqnorm(datos$ZTENERGI, main="ZTENERGI", pch=1, frame = FALSE)
qqline(datos$ZTENERGI, col = 'blue', lwd=2)
qqnorm(datos$ZPSERVI, main="ZPSERVI", pch=1, frame = FALSE)
qqline(datos$ZPSERVI, col = 'blue', lwd=2)
qqnorm(datos$ZPAGRICU, main="ZPAGRICU", pch=1, frame = FALSE)
qqline(datos$ZPAGRICU, col = 'blue', lwd=2)
qqnorm(datos$ZTMEDICO, main="ZTMEDICO", pch=1, frame = FALSE)
qqline(datos$ZTMEDICO, col = 'blue', lwd=2)
qqnorm(datos$ZESPVIDA, main="ZESPVIDA", pch=1, frame = FALSE)
qqline(datos$ZESPVIDA, col = 'blue', lwd=2)
qqnorm(datos$ZTMINFAN, main="ZTMINFAN", pch=1, frame = FALSE)
qqline(datos$ZTMINFAN, col = 'blue', lwd=2)
qqnorm(datos$ZPOBDENS, main="ZPOBDENS", pch=1, frame = FALSE)
qqline(datos$ZPOBDENS, col = 'blue', lwd=2)
qqnorm(datos$ZPOBURB, main="ZPOBURB", pch=1, frame = FALSE)
qqline(datos$ZPOBURB, col = 'blue', lwd=2)
```

### Test de Saphiro-Wilk

```{r echo=TRUE, include=TRUE, warning=FALSE}
datos_tidy <- melt(datos, value.name = "valor")
aggregate(valor ~ variable, data = datos_tidy,
          FUN = function(x){shapiro.test(x)$p.value})
#x>0.05 -> ZPOBURB, ZPSERVI, ZTEJERCI, ZTPOBACT 
qqnorm(datos$ZPOBURB, pch=1, frame = FALSE)
qqline(datos$ZPOBURB, col = 'blue', lwd=2)
```

## *Otras cuestiones de interés*

# **Análisis exploratorio multivariante**

## *Correlación*

```{r echo=TRUE, include=TRUE, warning=FALSE}
cor(datos)
corrplot(cor(datos), order = "hclust", tl.col='black', tl.cex=1)
```

La prueba de esfericidad de Bartlett contrasta la hipótesis nula de que la matriz 
de correlaciones es una matriz identidad, en cuyo caso no existirían correlaciones 
significativas entre las variables y el modelo factorial no sería pertinente. 

```{r echo=TRUE, include=TRUE, warning=FALSE}
cortest.bartlett(datos) #https://rpubs.com/PacoParra/293407
```

## *Valores perdidos*

## *Posibilidad de reducción de la dimensión*

### *Mediante variables observables*
  
  
```{r echo=TRUE, include=TRUE, warning=FALSE}
  #CODIGO PARA ACP

# La funci?n "prcomp" del paquete base de R realiza este an?lisis
# Pasamos los par?metros "scale" y "center" a TRUE para consideras
# los datos originales normalizados
PCA<-prcomp(datos, scale=T, center = T)
# El el campo "rotation" del objeto "PCA" es una matrix cuyas columnas
# son los coeficientes de las componentes principales, es decir, el
# peso de cada variable en la correspondiente componente principal
PCA$rotation
# En el campo "sdev" del objeto "PCA" y con la funci?n summary aplicada
# al objeto, obtenemos informaci?n relevante: desviaciones t?picas de 
# cada componente principal, proporci?n de varianza explicada y acumlada.
PCA$sdev
summary(PCA)


# Instalaci?n del paquete desde un repositorio en caso de no estar instalado
install.packages("ggplot2")
# Carga del paquete "ggplot2" si est? instalado
library("ggplot2")

# El siguiente gr?fico muestra la proporci?n de varianza explicada
varianza_explicada <- PCA$sdev^2 / sum(PCA$sdev^2)
ggplot(data = data.frame(varianza_explicada, pc = 1:11),
       aes(x = pc, y = varianza_explicada, fill=varianza_explicada )) +
  geom_col(width = 0.3) +
  scale_y_continuous(limits = c(0,0.6)) + theme_bw() +
  labs(x = "Componente principal", y= " Proporci?n de varianza explicada")

# El siguiente gr?fico muestra la proporci?n de varianza explicada
varianza_acum<-cumsum(varianza_explicada)
ggplot( data = data.frame(varianza_acum, pc = 1:11),
        aes(x = pc, y = varianza_acum ,fill=varianza_acum )) +
  geom_col(width = 0.5) +
  scale_y_continuous(limits = c(0,1)) +
  theme_bw() +
  labs(x = "Componente principal",
       y = "Proporci?n varianza acumulada")

# -------------------------------------------------------------- #
# Paso 4: selecci?n del n?mero de componentes principales ?ptimo #
# -------------------------------------------------------------- #
# Existen diferentes m?todos:
# 1.- M?todo del codo (Cuadras, 2007). Ejercicio: buscar informaci?n (voluntario)
# 2.- A criterio del investigador que elige un porcentaje m?nimo de varianza explicada
# por las componentes principales (no es fiable porque puede dar m?s de las necesarias.
# 3.- En este caso se utiliza la regla de Abdi et al. (2010). Se promedia las varianzas
# explicadas por la componentes principales y se selecciona aquellas cuya proporci?n de 
# varianza explicada supera la media.
# En este caso se eligen tan solo cuatro/cinco direcciones principales tal y como se puede ver
# que acumulan casi más del 90% de varianza explicada
PCA$sdev^2
mean(PCA$sdev^2)

# Cada componente principal se obtiene de forma sencilla como combinaci?n lineal de todas
# las variables con los coeficientes que indican las columnas de la matriz de rotaci?n
# Ejercicio voluntario: escribir la expresi?n anal?tica de cada componente princial

# -------------------------------------------------------------- #
# Paso 5: Representaci?n gr?fica de las componentes principales  #
# -------------------------------------------------------------- #
# El paquete "factoextra" permite la representaci?n de las componentes principales
# junto con las variables y observaciones del an?lisis de componentes principales.

# Instalaci?n del paquete desde un repositorio en caso de no estar instalado
# El siguiente paquete requiere tener al menos la vesri?n de R 4.0.x
install.packages("factoextra")
# Carga del paquete "factorextra" si est? instalado
library("factoextra")
# Esto produce una comparativa entre la primera y segunda componente principal analizando 
# que variables tienen m?s peso para la definici?n de cada componente principal
fviz_pca_var(PCA,
             repel=TRUE,col.var="cos2",
             legend.title="Distancia")+theme_bw()

# Esto produce una comparativa entre la primera y tercera componente principal analizando 
# que variables tienen m?s peso para la definici?n de cada componente principal
fviz_pca_var(PCA,axes=c(1,3),
             repel=TRUE,col.var="cos2",
             legend.title="Distancia")+theme_bw()

# Esto produce una comparativa entre la segunda y tercera componente principal analizando 
# que variables tienen m?s peso para la definici?n de cada componente principal
fviz_pca_var(PCA,axes=c(2,3),
             repel=TRUE,col.var="cos2",
             legend.title="Distancia")+theme_bw()


fviz_pca_var(PCA,axes=c(1,4),
             repel=TRUE,col.var="cos2",
             legend.title="Distancia")+theme_bw()

fviz_pca_var(PCA,axes=c(2,4),
             repel=TRUE,col.var="cos2",
             legend.title="Distancia")+theme_bw()

fviz_pca_var(PCA,axes=c(3,4),
             repel=TRUE,col.var="cos2",
             legend.title="Distancia")+theme_bw()






# Es posible tambi?n representar las observaciones de los objetos junto con las componentes 
# principales mediante la orden "contrib" de la funci?n "fviz_pca_ind", as? como identificar
# con colores aquellas observaciones que mayor varianza explican de las componentes principales

# Observaciones en la primera y segunda componente principal
fviz_pca_ind(PCA,col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var")+theme_bw()

# Observaciones en la primera y tercera componente principal
fviz_pca_ind(PCA,axes=c(1,3),col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var")+theme_bw()

# Observaciones en la segunda y tercera componente principal
fviz_pca_ind(PCA,axes=c(2,3),col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var")+theme_bw()


fviz_pca_ind(PCA,axes=c(1,4),col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var")+theme_bw()


fviz_pca_ind(PCA,axes=c(2,4),col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var")+theme_bw()


fviz_pca_ind(PCA,axes=c(3,4),col.ind = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel=TRUE,legend.title="Contrib.var")+theme_bw()


# Representaci?n conjunta de variables y observaciones
# que relaciona visualmente las posibles relaciones entre las
# observaciones, las contribuciones de los individuos a las varianzas de las componentes
# y el peso de las variables en cada componentes principal

# Variables y observaciones en las 1?  y 2? componente principal
fviz_pca(PCA,
         alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE,
         legend.title="Distancia")+theme_bw()

# Variables y observaciones en las 1?  y 3? componente principal
fviz_pca(PCA,axes=c(1,3),
         alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE,
         legend.title="Distancia")+theme_bw()

# Variables y observaciones en las 1?  y 2? componente principal
fviz_pca(PCA,axes=c(2,3),
         alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE,
         legend.title="Distancia")+theme_bw()


fviz_pca(PCA,axes=c(1,4),
         alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE,
         legend.title="Distancia")+theme_bw()


fviz_pca(PCA,axes=c(2,4),
         alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE,
         legend.title="Distancia")+theme_bw()


fviz_pca(PCA,axes=c(3,4),
         alpha.ind ="contrib", col.var = "cos2",col.ind="seagreen",
         gradient.cols = c("#FDF50E", "#FD960E", "#FD1E0E"),
         repel=TRUE,
         legend.title="Distancia")+theme_bw()



# Por ?ltimo, ya que el objeto de este estudio era reducir la dimensi?n de las variables
# utilizadas, es posible obtener las coordenadas de los datos originales tipificados en el
# nuevo sistema de referencia.
# De hecho lo tenemos almacenado desde que utilizamos la funci?n prcomp para crear la variable PCA

head(PCA$x,n=4)
```

  
  
  
  
  

### *Mediante variables latentes*
  
                                       
```{r echo=TRUE, include=TRUE, warning=FALSE}                     
 #AF                                         
                                          
library(polycor)

poly_cor<-hetcor(datos)$correlations

# Hay que escoger un m?todo para extraer los factores, ACP, verosimilitud, etc.
# La funci?n fa() implementa hasta 6 m?todos distintos
# Vamos a comparar las salidas con el m?todo del factor principal y con el de 
# m?xima verosimilitud.

### prueba de dos modelos con tres factores
modelo1<-fa(poly_cor,
            nfactors = 4,
            rotate = "none",
            fm="mle") # modelo m?xima verosimilitud

modelo2<-fa(poly_cor,
            nfactors = 4,
            rotate = "none",
            fm="minres") # modelo m?nimo residuo
# comparando las comunalidades
sort(modelo1$communality,decreasing = T)->c1
sort(modelo2$communality,decreasing = T)->c2
head(cbind(c1,c2))

# comparacion de las unicidades, es decir la proporción de varianza
# que no ha sido explicada por el factor (1-comunalidad)
sort(modelo1$uniquenesses,decreasing = T)->u1
sort(modelo2$uniquenesses,decreasing = T)->u2
head(cbind(u1,u2))

# Determinemos ahora el n?mero ?ptimo de factores
# Hay diferentes criterios, entre los que destacan el Scree plot (Cattel 1966)
# y el an?lisis paralelo (Horn 1965). Ejercicio: buscar sus interpretaciones
scree(poly_cor)
fa.parallel(poly_cor,n.obs=200,fa="fa",fm="minres")


# Estimamos el modelo factorial con 5 factores implementando una rotaci?n
# tipo varimax para buscar una interpretaci?n m?s simple.
modelo_varimax<-fa(poly_cor,nfactors = 2,rotate = "varimax",
                   fa="mle")

# Mostramos la matriz de pesos factorial rotada
print(modelo_varimax$loadings,cut=0) 
# Visualmente podr?amos hacer el esfuerzo de ver con qu? variables correlacionan cada 
# uno de los factores, pero es muy tedioso de modo que utilizamos la siguiente
# representaci?n
fa.diagram(modelo_varimax)
# En este diagrama, entre otras cosas se ve que el primer factor esta asociado 
# con los items E1, E2, E3, E4, E5 y N4, que son los items del cuestioario que 
# tratan de identificar la cualidad de la extraversi?n



# ----------------------------------------------------
# Otra forma de hacerlo, con test de hip?tesis al final
# que contrasta si el numero de factores es suficiente 
library(stats)
factanal(datos,factors=4, rotation="none")

```

## *Análisis de normalidad*

## *Construcción de un clasificador*

## *Validación de los clasificadores*
